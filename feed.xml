<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://warrenzha.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://warrenzha.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-12-01T20:58:07+00:00</updated><id>https://warrenzha.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Software-defined, AI-driven Dyanmic mmWave Networking</title><link href="https://warrenzha.github.io/blog/2023/mesh-net/" rel="alternate" type="text/html" title="Software-defined, AI-driven Dyanmic mmWave Networking" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2023/mesh-net</id><content type="html" xml:base="https://warrenzha.github.io/blog/2023/mesh-net/"><![CDATA[<p><em>Abstract:</em> Integrated Access and Backhaul (IAB) is an emerging technique to enable cost-effective deployment of dense 5G networks that utilize emerging millimeter-wavelength (mmWave) spectrum. Existing heuristic-based network control/management frameworks are not well-suited for the increasing complexity and uncertainty introduced by mmWave IAB. Machine learning (ML) can help automate network control decisions, but its practical deployment faces new system-level challenges in 5G IAB, including accurate simulation-based training, resolving conflicting objectives from heterogeneous network slices, and efficiently collecting observations for run-time decision making. In this paper, we develop a general framework for effectively deploying reinforcement learning (RL) to control 5G IAB networks. Our framework incorporates a data-driven stochastic simulation scheme to bridge the simulation-to-reality gap, a piecewise reward shaping mechanism to handle competing conflicting performance objectives, and a simple observation selection algorithm to reduce the input size into the RL policy. We validate this framework using real-world network measurements from a mmWave IAB testbed, combined with a large scale ray tracing simulation. Experiments on a set of challenging 5G IAB network control problems demonstrate the effectiveness of our framework to enable practical RL integration into 5G IAB.</p>

<p><a href="https://wyzhao030.github.io/assets/pdf/AIdriven_Dynamic_MillimeterWave_Mesh.pdf">pdf</a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><category term="Communication" /><summary type="html"><![CDATA[We develop a general framework for effectively deploying reinforcement learning (RL) to control 5G mmWave IAB networks.]]></summary></entry><entry><title type="html">GAN-based EEG Signal Generation</title><link href="https://warrenzha.github.io/blog/2022/eeg-gen/" rel="alternate" type="text/html" title="GAN-based EEG Signal Generation" /><published>2022-04-20T14:14:00+00:00</published><updated>2022-04-20T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2022/eeg-gen</id><content type="html" xml:base="https://warrenzha.github.io/blog/2022/eeg-gen/"><![CDATA[<p><em>Abstract:</em> Processing and analysis of brain signals generally requires a large amount of data. But the acquisition of EEG signals is difficult while the sample size of the data set is small, and sometimes categories are unbalanced in the data set. Based on the challenge, we proposed the WGAN-GP method, a variant of GAN, to generate useful EEG signals. The experiments on single-channel and multi-channel model both show that the performance of WGAN-GP is stable, and the generated signals have close shape with the real signals, and have better spectrum performance than traditional methods. Our results and analysis show that WGAN-GP can generate accurate and diverse EEG signals, and thus, help extend the data set which is difficult to collect physically. We’ve made the code associated with this work available at https://github.com/warrenzha/GAN-EEG-generation.</p>

<p><a href="https://wyzhao030.github.io/assets/pdf/GAN-EEG-Generation.pdf">pdf</a></p>

<p><a href="https://github.com/warrenzha/GAN-EEG-generation">link</a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><summary type="html"><![CDATA[We proposed the WGAN-GP method, a variant of GAN, to extend data set of EEG signals.]]></summary></entry><entry><title type="html">Blind Deconvolution Using Convex Programming</title><link href="https://warrenzha.github.io/blog/2022/blind-decov/" rel="alternate" type="text/html" title="Blind Deconvolution Using Convex Programming" /><published>2022-02-01T14:14:00+00:00</published><updated>2022-02-01T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2022/blind-decov</id><content type="html" xml:base="https://warrenzha.github.io/blog/2022/blind-decov/"><![CDATA[<p><em>Abstract:</em> We study the question of recovering two signals w and x from their convolution y = w ∗ x. Generally, the solution to this blind deconvolution problem is non-unique and non-convex. But with assumptions on sparsity, subspace structure and transformed variable, we can convert the non-convex nuclear norm into a convex problem by ”dual-dual” relaxation. In this project, we also implement the convex algorithm proposed in Blind Deconvolution Using Convex Programming, and compare its performance with non-blind and non-convex algorithms. Moreover, the evaluation shows that the convex algorithm is robust against sparsity violation, but sensitive to low-rank condition. At last, we try to extend the algorithm to 2D deconvolution by recovering a blurred image. But the result on 2D deconvolution still need improvement.</p>

<p><a href="https://github.com/warrenzha/blind-deconvolution">link</a></p>]]></content><author><name></name></author><category term="Optimization" /><summary type="html"><![CDATA[Implementation of blind deconvolution using convex programming.]]></summary></entry><entry><title type="html">ML-based Matrix Optimization in Massive MIMO</title><link href="https://warrenzha.github.io/blog/2021/cvgnn/" rel="alternate" type="text/html" title="ML-based Matrix Optimization in Massive MIMO" /><published>2021-07-04T14:14:00+00:00</published><updated>2021-07-04T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2021/cvgnn</id><content type="html" xml:base="https://warrenzha.github.io/blog/2021/cvgnn/"><![CDATA[<p><em>Abstract:</em> In the downlink of massive MIMO, the transmitter uses precoding technology to reduce interference and improve spectrum efficiency. In this paper, a complex-valued gradient neural network (CVGNN) is proposed to solve the Moore-Penrose inversion of the complex matrix used in massive MIMO precoding algorithms.</p>

<p><a href="https://wyzhao030.github.io/assets/pdf/ML_Matrix_Optimization_MIMO.pdf">pdf</a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><category term="Communication" /><summary type="html"><![CDATA[A complex-valued gradient neural network (CVGNN) is proposed to solve the Moore-Penrose inversion of complex matrices.]]></summary></entry><entry><title type="html">ML-based Customer Review Classification</title><link href="https://warrenzha.github.io/blog/2020/review-class/" rel="alternate" type="text/html" title="ML-based Customer Review Classification" /><published>2020-11-02T14:14:00+00:00</published><updated>2020-11-02T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2020/review-class</id><content type="html" xml:base="https://warrenzha.github.io/blog/2020/review-class/"><![CDATA[<p><em>Abstract:</em> Customer reviews on e-commerce platforms contain valuable information, while sifting through them manually tends to dismay people because of the huge amount of data. This study implemented a machine learning-based algorithm to classify customer reviews. Our classifier extracts Chinese word segmentation and text frequency for feature extraction and scoring, and implements the classification with methods of Naive Bayesian and Support Vector Machines. Experimental results on the Taobao product review sentiment datasets show that our model based on two machine learning algorithms, though results in different performances, can provide suggestions on the selection of the identification classifier using a trade-off strategy and helps obtain fast and accurate classification on reviews of different categories.</p>

<p><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1678/1/012081/pdf">pdf</a></p>]]></content><author><name></name></author><category term="Machine-Learning" /><summary type="html"><![CDATA[This study implemented a machine learning-based algorithm to classify customer reviews.]]></summary></entry><entry><title type="html">Beam Alignment and Tracking for Millimeter Wave Communications via Bandit Learning</title><link href="https://warrenzha.github.io/blog/2020/bat/" rel="alternate" type="text/html" title="Beam Alignment and Tracking for Millimeter Wave Communications via Bandit Learning" /><published>2020-09-28T14:14:00+00:00</published><updated>2020-09-28T14:14:00+00:00</updated><id>https://warrenzha.github.io/blog/2020/bat</id><content type="html" xml:base="https://warrenzha.github.io/blog/2020/bat/"><![CDATA[<p><em>Abstract:</em> Millimeter wave (mmwave) communications have attracted increasing attention thanks to the abundant spectrum resource. The short wave-length of mmwave signals facilitates exploiting large antenna arrays to achieve large array gains and combat large path-loss. However, the use of large antenna arrays along with narrow beams leads to a large overhead in beam training for obtaining channel state information, especially in dynamic environments. To reduce the overhead of beam training, in this paper we formulate the problem of beam alignment and tracking (BA/T) as a stochastic bandit problem. In particular, to sense the change of the environments, the actions are designed based on the offset of successive beam indexes (i.e., beam index difference), which measures the rate of change of the environments. Then, we propose two efficient BA/T algorithms based on the stochastic bandit learning. To reveal useful insights, the performance of effective achievable rate is further analyzed for the proposed BA/T algorithms. The analytical results show that the algorithms can sense the change of the environments and adjust beam training strategies intelligently. In addition, they do not require any priori knowledge of dynamic channel modeling, and thus are applicable to a variety of complicated scenarios. Simulation results demonstrate the effectiveness and superiority of the proposed algorithms.</p>]]></content><author><name></name></author><category term="Communication" /><category term="Optimization" /><summary type="html"><![CDATA[We formulated the problem of beam alignment and tracking (BA/T) as a stochastic bandit problem.]]></summary></entry></feed>