---
layout: post
title: Sparsely Activated BNNs from Deep Gaussian Process
date: 2024-06-10 10:14:00-0400
description: We developed a sparse expansion of deep Gaussian processes as Bayesian neural networks that are amenable to efficient training and prior design.
tags: Machine-Learning
# categories: coding
giscus_comments: true
related_posts: false
toc:
  sidebar: left
---

_Theory and algorithm development:_ Building on our initial success, we aim to develop a deeper understanding of the nature of the DTMGP models and construct a broader class of DTMGP models with more complex architectures. This will be mainly accomplished by exploring various equivalent expansions of TMGPs and architectures with skip connections or feedback, akin to ResNet or RNN in the deterministic setting. Furthermore, to address the challenge of prior design, we plan to work to understand how the priors (especially, the covariance kernel functions) of the GP units impact the prior of the overall DGP model.

_Applications to conditional generative tasks:_ While the primary motivation for studying DGPs in the literature so far has been uncertainty quantification for learning supervised classification/regression, the stochastic nature of these models also makes them natural candidates as generative models. While the general properties of DGPs as generative models are largely unknown, we are encouraged by our preliminary results on applying the DTMGP model to the MNIST dataset_ and the structural similarity between DGPs and the immensely popular Gaussian diffusion models.

_Software development and validation:_ We plan to develop software tools that implement the proposed sparsely activated BNNs, which can be used for both regression and generative tasks. The software will be open sourced under the GNU-GPL license, and will be validated using real-world datasets from the state-of-the-art wireless communication and computer vision systems.